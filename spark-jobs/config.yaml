# Spark Job Configuration

# Spark Application Settings
spark:
  app_name: retail-streaming-job
  master: yarn
  deploy_mode: cluster
  driver_memory: 4g
  executor_memory: 4g
  executor_cores: 2
  num_executors: 3
  max_retries: 3
  shuffle_partitions: 200

# Kafka Configuration
kafka:
  bootstrap_servers: localhost:9092
  topics:
    transactions: retail-transactions
    inventory: retail-inventory
    customers: retail-customers
    products: retail-products
  consumer_group: retail-spark-group
  starting_offsets: latest
  fail_on_data_loss: false

# PostgreSQL Configuration
postgres:
  host: localhost
  port: 5432
  database: retail_db
  user: ${POSTGRES_USER}
  password: ${POSTGRES_PASSWORD}
  tables:
    transactions: retail_transactions
    inventory: retail_inventory
    customers: retail_customers
    products: retail_products

# Processing Settings
processing:
  batch_interval: 60
  watermark_delay: 10
  checkpoint_location: /tmp/checkpoints
  output_mode: append
  trigger_interval: 1 minute

# Data Quality Rules
data_quality:
  rules:
    - name: valid_transaction_amount
      condition: "price > 0 AND quantity > 0"
      action: filter
    - name: valid_customer_id
      condition: "customer_id IS NOT NULL"
      action: filter
    - name: valid_product_id
      condition: "product_id IS NOT NULL"
      action: filter

# Monitoring
monitoring:
  metrics:
    - name: processed_records
      type: counter
    - name: processing_time
      type: gauge
    - name: error_count
      type: counter
  prometheus:
    enabled: true
    port: 9090

# Logging
logging:
  level: INFO
  file: spark_job.log
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  max_file_size: 100MB
  max_files: 10 